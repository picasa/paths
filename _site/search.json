[
  {
    "objectID": "posts/paper/index.html",
    "href": "posts/paper/index.html",
    "title": "Papers",
    "section": "",
    "text": "A selection of my digital works that are traced on paper using a pen-plotter (AxiDraw A3/V3)."
  },
  {
    "objectID": "posts/asemic/index.html",
    "href": "posts/asemic/index.html",
    "title": "Asemic",
    "section": "",
    "text": "Asemic characters and words generated with splines controlled by random control-points.\n\nThis serie was developed after reading Anders Hoff works on asemic writing. In his algorithm, individual shapes (glyphs) are generated with splines defined by few control points sampled from a 2D space (square, ellipse), and glyphs are concatenated together to create cursive-like writing.\nHere, to produce words or paragraphs with sensible text-like aesthetics, I limited the number of generated glyphs and mapped them to a fixed set of characters: the represented glyph sequence is a function of a generated text (e.g. Lorem ipsum).\nVariation can be obtained by acting on the glyph aggregation method. If the glyphs are drawn independently from each other, the algorithm emulates script writing. More detailed symbols can be obtained by concatenating a few glyphs (2-4) before mapping them to characters. Increasing the number of spline control points also produces refined shapes, but quickly becomes too complex. The examples below are using two random seeds: one for the character set, and one for the text sequence used as layout."
  },
  {
    "objectID": "posts/cells/index.html",
    "href": "posts/cells/index.html",
    "title": "Cells",
    "section": "",
    "text": "Cellular-like features generated from tessellation of point sets.\n\nThis algorithm first generates a point set, using either a deterministic (ODE) or stochastic method. Then, a space tessellation method is used on a subset of the original point set. This method is used for different features loosely inspired by cellular biology (e.g. cells, tissues, vessels), that are assembled as different layers.\nThe aim was to search for a balance between the algorithm capacity to generate various outputs (but needing a curation step) and its capacity to produce aesthetic but overly similar outputs (narrow parameterization). This solution also explored the variability generated by sampling the same initial point set at different scales and locations. This last idea was adapted to the sampling of various topographical locations in the dispyr series."
  },
  {
    "objectID": "posts/ridges/index.html",
    "href": "posts/ridges/index.html",
    "title": "Ridges",
    "section": "",
    "text": "Distinct and shifted lines visualized from digital elevation model datasets.\n\nThe central idea is to convert 3D point sets to 2D lines. Digital Elevation Model (DEM) represents elevation as function of latitude and longitude. In France, the national geographic institute makes this DEM available in Open-Data (RGE, up to a 1m resolution). These illustrations are made of multiple lines of elevation as a function of longitude, for discrete latitude values. Overplotting is avoided (hidden lines) by checking for a minimal distance between two lines. Variation comes from the scale and location of the geographical region and from the parameterization of the line clipping method.\nAdding noise or removing information from the original DEM data seems to create more a more natural rendering. This option was explored in the dispyr series"
  },
  {
    "objectID": "posts/collatz/index.html",
    "href": "posts/collatz/index.html",
    "title": "Collatz",
    "section": "",
    "text": "Curves parameterized from integer sequences and positioned according to basic botanical rules.\n\nThis study focus on a very simple generator, here a sequence of numbers generated from a set of rules. In this case, I used the Collatz sequence because it has a decreasing, random-looking pattern, which is a nice feature for plant structures (e.g. the internode distribution on a stem).\nThis sequence is used to define a curve, by mapping sequence elements to segment length, with a random angle between segments. Multiple curves are computed to constitute a node, and multiple nodes to constitute a stem / individual. Further recursion can be used to create populations of multiple individuals, with random or community-dependent features.\nThe main random elements are the initial value of the sequence, and the angle distribution between curve segments.\nOn this serie, rather than properly computing hidden line or polygons, I used an external algorithm (bitmap tracing) which had the nice side-effect of reshaping and merging some paths."
  },
  {
    "objectID": "posts/dispyr/index.html",
    "href": "posts/dispyr/index.html",
    "title": "Dispyr",
    "section": "",
    "text": "This work is an attempt to emulate etching drawings with code, by playing with noise and data removal to create a natural rendering of random fragments of the Pyrénées.\nI was searching for an hybrid between the simplicity and efficiency of a famous data visualization in astronomy and the impressive Meridian serie by Matt Deslauriers. Moreover, to be able to use a pen-plotter to trace the images on paper, the algorithm should produce relatively long lines rather than the myriad of short segments that make Meridian appealing.\n\n\nleft : “Successive pulses from the first pulsar discovered, CP 1919, are here superimposed vertically. The pulses occur every 1.337 seconds. They are caused by rapidly spinning neutron star.” From The Cambridge Encyclopaedia of Astronomy.right : Meridian #638\n\nThis algorithm feeds on elevation data produced by public structures with different technologies (image analysis, topographical radar, or lidar) and made available for reuse. In this sense, it differs from generative art, where the code is self-sufficient to produce results (see why-love-generative-art) This short article explains the underlying logic and software code. A basic reproducible example is available here to encourage creative experiments.\nI won’t comment much on the choice of the R language and the code itself. It’s just that I knew this language for work, and I was curious to see if i could reuse it for other projects. For me, the functional programming paradigm makes sense to interact with code, moreover when working in interrupted streaks. The broad data exploration workflow resonate with my somewhat reductionist creative approach, especially how the tools for data visualization enable to focus on visual experiments rather code for graphics. I also went for the easy way, thanks to the creators of incredible libraries for (spatial) data processing and graphics. For this project I used {{stars}}, {{dplyr}}, and {{ggplot2}}.\nThe algorithm is composed by three main functions, created to be used successively (with the R pipe function, |>). This article broadly illustrates the content of these functions.\n\nCodeget_features_ridge(seed = 15) |> gen_dispyr(n_ridges = 800) |> plot_ridge()\n\n\n\n\nget_features_ridge() is a function of a random seed, and is used to generate the main features of the output (e.g. the location, orientation, style)\n\ngen_dispyr() uses this set of features to download and process data from a digital elevation model (3D) into a set of lines (2D).\n\nplot_ridge() renders the processed data into a vector image."
  },
  {
    "objectID": "posts/dispyr/index.html#features-generation",
    "href": "posts/dispyr/index.html#features-generation",
    "title": "Dispyr",
    "section": "Features generation",
    "text": "Features generation\nThe outputs have only two broad features: a random location in the Pyrénées mountain range and a rendering style, randomly chosen among four methods with a set of probabilities. Most of the visual variability in the outputs is driven by the choice of geographical location, it gives the series a strong link with the subject, but somehow limits the maximum number of obtainable iterations.\nLocation\nTo get a random location in the Pyrénées mountain range, we first define a sampling region (a 20 km wide buffer around the French-Spain border), and use sf::st_sample to sample a location in this polygon. The code is simplified to sample a mysterious location in a polygon defined by the Andorra borders.\n\nCode# load a polygon of the Andorra borders\npolygon <- rnaturalearth::ne_countries(\n  country = c(\"andorra\"), scale = \"medium\", returnclass = \"sf\"\n  ) |> st_transform(2154)\n\n# get a sample in the defined polygon \ncoord <- st_sample(polygon, size = 1) |> st_as_sf()\n\n\n\n\n\n\n\n\n\nRendering style\nThe overall rendering style is impacted by a dozen parameters in the code (e.g. line density, position of lines in the y-axis, amount and position of removed data). Rather than a free exploration of this parameter space, which could lead to heterogeneous outputs, I defined a limited number of fixed sets of parameters. I created four styles, and named them as weather conditions (clear, mist, snow, storm).\nTechnically nothing complicated, the get_features_ridge() function returns a list of features and associated parameters, later used for the data processing step. The parameters are presented at the moment they are used in the code."
  },
  {
    "objectID": "posts/dispyr/index.html#data-processing",
    "href": "posts/dispyr/index.html#data-processing",
    "title": "Dispyr",
    "section": "Data processing",
    "text": "Data processing\nThis stage uses the previous set of features to download and process data from a digital elevation model (3D) into a set of lines (2D), with distinct processing steps.\nGetting a realistic description of the landscape\nThis step start by actually retrieving the elevation data around the sampled location (a 2x2 km square for the illustration, 20x15 km normally). The {{elevatr}} R package makes this step a breeze, but the process is the same when using local high-resolution data.\n\nCode# get DEM data in a 2x2 km region around the sampled point\ndem <- elevatr::get_elev_raster(\n  coord, z = 11, expand = 1E3, clip = \"bbox\") \n\n# use a heatmap to glance at elevation data\nplot <- ggplot() +\n  geom_stars(data = st_as_stars(dem)) +\n  geom_sf(data = coord, color = \"white\", size = 1) +\n  scale_fill_viridis_c(name = \"elevation\") +\n  labs(x = \"lon\", y = \"lat\") \n\n\n\n\n\n\n\n\n\nSlicing the landscape into lines\nThe aim is to process a 3D point set into 2D lines, by computing multiple lines of elevation as a function of longitude, for discrete latitude values. This step is perfectly illustrated by the stacked plot of radio signals presented previously. In our case, each line corresponds to one row in the elevation data (74x74 matrix in the example). Without further modifications, the output is unclear, with a lot of intersecting lines (figure A).\nTo improve the output readability, but also introduce potential for variations, we process this raw elevation matrix with two actions:\n\nshift lines in the y-direction as a function of their rank from the foreground.\ndelete the line segments that should be hidden by foreground lines with a higher elevation.\n\n\nCode# parameters\nz_shift = 5\n\n# convert DEM from spatial to dataframe format\ndf_dem <- dem |> stars::st_as_stars() |>\n  as_tibble() |> select(x, y, z = 3)  \n\n# compute elevation shift as a function of normalized distance\ndata_index <- df_dem |>\n  distinct(y) |> arrange(y) |>\n  mutate(\n    y_rank = rank(y),\n    y_dist = scales::rescale(y, to = c(0,1)),\n  ) |>\n  mutate(dz = y_rank * z_shift)\n\n# compute shifted elevation values\ndf_shift <- df_dem |> \n  left_join(data_index, by = \"y\") |>\n  group_by(y) |> mutate(xn = x - min(x), x_rank = rank(x)) |> ungroup() |>\n  mutate(zs = z + dz)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shifting step calculates the difference from the reference elevation as a linear function of its rank from the foreground (figure B). Increasing the slope of this function impacts the perspective, towards a more aerial view of the landscape, eventually reducing the number of intersecting lines (figure C). Using non-linear functions creates unrealistic but interesting perspectives.\nAt this step, a fast solution to achieve line intersection is by drawing lines with a filled area underneath, starting from the background and thus progressively masking previous polygons (as in painter’s algorithm). This solution is implemented directly in the ggridges R package. But this option is not ideal for creative coding : the hidden lines still exist, which makes vector outputs unusable with a pen-plotter, and it lacks flexibility on the overlaying method.\n\nCode# parameters\nn_lag = 100\nz_threshold = 0\n\n# define window functions for multiple lag positions\nlist_distance <- map(glue::glue(\"~ . - lag(., n = {1:n_lag})\"), ~ as.formula(.))\nlist_col <- glue::glue(\"zs_{1:n_lag}\")\n\n# remove points hidden by foreground ridges :\n# compute lagged elevation difference between successive y for each x \n# replace value by NA when the difference is less than a threshold \ndf_ridge <- df_shift |> \n  arrange(y_rank) |> group_by(x) |>\n  mutate(across(zs, .fns = list_distance)) |>\n  ungroup() |> \n  mutate(\n    zn = case_when(\n      if_any(all_of(list_col), ~ . < z_threshold) ~ NA_real_,\n      TRUE ~ zs)) |>\n  select(- all_of(list_col))\n\n\nIn the code above, the intersections between lines are avoided by checking for a minimum distance between a point at a given x position and its neighbourhood along y, and removing the point if any points in the neighbourhood falls below this distance (z_threshold parameter). To avoid computing the distance matrix for all points (744, in this small area), we only check for intersections along a limited range (n_lag parameter, between 0 and 300 y ranks). Increasing the threshold parameter creates clearer distinction between the sides of mountains.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLine segments are hidden by foreground lines by using polygon masking (left) or distance-based point removal with a null (center) or positive threshold (right).\n\n\n\n\n\n\nEffect of shift (in rows) and threshold (in columns) parameter combination on the landscape rendering (16 km2).\n\nAltering the lines with noise and deletion\nFinally, the overall aesthetic is obtained by adding noise and discarding data as a function of various attributes (elevation, slope, elevation dispersion). The governing idea was to test how our perception of the landscape changes when most of the initial topographical data is perturbed or removed. Here are four methods, but much is left to explore. The illustration code here is functioning without external call, but highly redundant and surely not optimal, you can conveniently hide it if you feel horrified.\n\n\n\n\n\nFour data processing methods, all based on filtering points and adding noise, and named after weather conditions (clear, mist, snow, storm).\n\nclear\nFor each ridgeline, the method samples 50 % of the points and adds moderate jitter on elevation values.\n\nCode# parameters\nz_remove = 0.5      # proportion of points to remove\nz_jitter = 4        # amount of jitter in elevation values\n\n# randomly sample a proportion of points in a line, jitter their y-position.\ndf_plot <- df_ridge |>\n  group_by(y_rank) |>\n  slice_sample(prop = (1 - z_remove)) |>\n  mutate(zn = jitter(zn, amount = z_jitter))\n\n# plot output\ndf_plot |> ggplot(aes(x, zn, group = y)) +\n  geom_line(alpha = 0.5) + coord_fixed() + theme_void() \n\n\n\n\nmist\nSame base as the previous method. In addition, when below an elevation threshold, lines are smoothed and points are randomly removed.\n\nCode# functions\n\n# add a proportion of missing values in a vector\nsample_missing <- function(x, p) {\n  n <- length(x)\n  s <- sample(1:n, size = p * n)\n  x[s] <- NA\n  return(x)\n}\n\n# parameters\nx_size = 10         # window of the rolling average \nz_limit = 0.5       # quantile value determining the elevation threshold\nz_remove = 0.5      # proportion of points to remove in lines\nz_missing = 0.2     # proportion of missing value to add\nz_jitter = 4        # amount of jitter in elevation values\n\n# calculate elevation cut\nz_cut = quantile(df_ridge$z, z_limit, na.rm = TRUE)\n\n# smooth lines with a rolling mean \ndf_smooth <- df_ridge |>\n  group_by(y_rank) |>\n  mutate(\n    zm = RcppRoll::roll_mean(zn, n = x_size, fill = NA),\n    zm = case_when(\n      (is.na(zm) & (x_rank < x_size/2 | x_rank > max(x_rank) - x_size/2)) ~ zn,\n      TRUE ~ zm)) \n\n# remove data as a function of elevation, jitter y-position above a threshold   \ndf_plot <- df_smooth |> \n  slice_sample(prop = (1 - z_remove)) |>\n  mutate(\n    zn = case_when(\n      (z < (z_cut - 200)) ~ sample_missing(zm, p = z_missing + 0.2),\n      (z < z_cut)         ~ sample_missing(zm, p = z_missing),\n      TRUE                ~ zn),\n    zn = if_else(z > z_cut, jitter(zn, amount = z_jitter), zn)\n  ) \n\n# plot output\ndf_plot |> ggplot(aes(x, zn, group = y)) +\n  geom_line(alpha = 0.5) + coord_fixed() + theme_void() \n\n\n\n\nsnow\nSame base as the previous method, but the data removal is also a function of a slope threshold.\n\nCode# parameters\nx_size = 5          # window of the rolling average \nz_limit = 0.7       # quantile value determining the elevation threshold\nz_remove = 0.5      # proportion of points to remove in lines\nz_missing = 0.6     # proportion of missing value to add\nz_jitter = 4        # amount of jitter in elevation values\n\n# calculate elevation cut\nz_cut = quantile(df_ridge$z, z_limit, na.rm = TRUE)\n\n# calculate local slope and remove points for flat areas\ndf_filter <- df_ridge |>\n    group_by(y_rank) |> arrange(x_rank) |>\n    mutate(\n      z_slope = abs(\n        (zn - lag(zn, default = 0)) / (xn - lag(xn, default = 0))\n        ),\n      z_slope = RcppRoll::roll_mean(z_slope, n = x_size, fill = NA_real_),\n      zn = if_else(z_slope == 0, NA_real_, zn)\n    ) |> ungroup()\n\n# remove data as a function of slope and elevation   \ndf_plot <- df_filter |>\n  group_by(y_rank) |>\n  slice_sample(prop = (1 - z_remove)) |>\n  mutate(\n    zn = case_when(\n      (z > z_cut) & (z_slope < 60/100)         ~\n        sample_missing(zn, p = z_missing),\n      (z > (z_cut - 100)) & (z_slope < 60/100) ~\n        sample_missing(zn, p = z_missing / 2),\n      TRUE                                     ~\n        zn),\n    zn = jitter(zn, amount = z_jitter)\n  ) \n\n# plot output\ndf_plot |> ggplot(aes(x, zn, group = y)) +\n  geom_line(alpha = 0.5) + coord_fixed() + theme_void() \n\n\n\n\nstorm\nWhole strips of lines are removed when the elevation variation is below a threshold. Additionally, an important jitter is added as a function of local slope value.\n\nCode# functions\n\n# scale the input vector with an exponential function\nf_exp <- function(x, k = 1, a = 1, b = 0, scale = FALSE) {\n  if (scale == TRUE) x = scales::rescale(x, to=c(0,1)) else x\n  a * exp(k * x) + b\n}\n\n# find multiple non-consecutive minima in a vector (this is awful) \nget_extremum <- function(data, n = 5, w = 20, delta = 20, method = \"max\") {\n  data |>\n    mutate(\n      y_smooth = RcppRoll::roll_mean(y_raw, n = w, na.rm = TRUE, fill = NA)\n    ) |>\n    filter((y_smooth - y_raw) > delta) |>\n    filter(\n      if_else(\n        lag(y_raw, n = 1) > y_raw & lead(y_raw, n = 1) > y_raw,\n        TRUE, FALSE)\n    ) |>\n    slice_min(y_raw, n = n)\n}\n\n# parameters\ny_range = 5        # number of ridges to remove around selected ones\nx_size = 5          # window of the rolling average \nz_remove = 0.5      # proportion of points to remove in lines\nz_jitter = 4        # amount of jitter in elevation values\n\n\n# get maximum number of ridgelines\nn_ridge_max <- df_ridge |> distinct(y_rank) |> nrow()\n\n# calculate local slope and remove points for flat areas\ndf_filter <- df_ridge |>\n    group_by(y_rank) |> arrange(x_rank) |>\n    mutate(\n      z_slope = abs(\n        (zn - lag(zn, default = 0)) / (xn - lag(xn, default = 0))\n        ),\n      z_slope = RcppRoll::roll_mean(z_slope, n = x_size, fill = NA_real_),\n      zn = if_else(z_slope == 0, NA_real_, zn)\n    ) |> ungroup()\n\n# compute potential distortion on z-axis as a function of local slope on x-axis\n# jitter by sampling in this distortion level for each point\ndf_sample <- df_filter |>\n  group_by(y_rank) |>\n  slice_sample(prop = (1 - z_remove)) |>\n  mutate(z_jitter = f_exp(z_slope, k = 3.5, a = 0.3, scale = TRUE) * z_jitter) |>\n  ungroup() |>\n  mutate(\n    z_jitter = runif(n(), -z_jitter, z_jitter),\n    zn = zn + z_jitter\n  )\n\n# get ridges position with low elevation variance\nindex_strip <- df_sample |>\n  group_by(y_rank) |>\n  summarise(y_raw = sd(z, na.rm = TRUE)) |>\n  get_extremum(n = (z_remove * 5), w = 30, delta = 3, method = \"min\") |>\n  filter(between(y_rank, (y_range + 5), (n_ridge_max - y_range - 5)))\n\n# remove strips of ridges based on previous index\ndf_plot <- anti_join(\n  df_sample,\n  df_sample |>\n    distinct(y_rank) |>\n    slice(\n      index_strip |> pull(y_rank) |>\n        map(~ (.x - y_range):(.x + y_range)) |>\n        flatten_int()\n    ),\n  by = \"y_rank\"\n)\n\n# plot output\ndf_plot |> ggplot(aes(x, zn, group = y)) +\n  geom_line(alpha = 0.5) + coord_fixed() + theme_void()"
  },
  {
    "objectID": "posts/dispyr/index.html#rendering",
    "href": "posts/dispyr/index.html#rendering",
    "title": "Dispyr",
    "section": "Rendering",
    "text": "Rendering\nThis step is essentially playing on opacity (either constant or variable along a line) and on the doubling of the line strokes to emulate pencil lines. Thanks to the R graphical devices, the same object can be exported to bitmap or vector files.\ndefault\n\nCode# parameters\nz_remove = 0.5      # proportion of points to remove\nz_jitter = 4        # amount of jitter in elevation values\np_alpha = 0.5       # mean opacity value\n\n# randomly sample a proportion of points in a line, jitter their y-position.\ndf_plot <- df_ridge |>\n  group_by(y_rank) |>\n  slice_sample(prop = (1 - z_remove)) |>\n  mutate(zn = jitter(zn, amount = z_jitter))\n\n# render lines with a constant alpha value\ndf_plot |> ggplot(aes(x, zn, group = y)) +\n  geom_line(alpha = p_alpha) + coord_fixed() + theme_void() \n\n\n\n\nvariable opacity\n\nCode# parameters\np_alpha_sd = 0.10   # dispersion of opacity value \n\n# render line with an alpha value sampled from a Gaussian distribution\ndf_plot |>\n  mutate(z_alpha = rnorm(n(), p_alpha, p_alpha_sd)) |> \n  ggplot(aes(x, zn)) +\n  geom_line(aes(group = y_rank, alpha = z_alpha)) +\n  scale_alpha_identity() + coord_fixed() + theme_void() \n\n\n\n\nvariable opacity and double strokes\n\nCode# functions\n# fit a polynomial determined by one or more numerical predictors\nf_loess <- function(data, span, n = 10) {\n\n  # do not fit model if less than n non-na values\n  if (sum(!is.na(data$zn)) < n) {\n\n    return(rep(NA, nrow(data)))\n\n  } else {\n\n    m <- loess(zn ~ xn, data = data, na.action = na.exclude, span = span)\n    return(predict(m))\n\n  }\n}\n\n# parameters\np_span = 0.5      # span of the smoothing model (smaller fits the line)\np_length = 25     # minimal line length to apply the smoothing model\n\n# render line with two strokes, and variable alpha\ndf_smooth <- df_plot |>\n  group_by(y_rank) |> nest() |>\n  mutate(z_smooth = map(data, ~ f_loess(., span = p_span, n = p_length))) |>\n  unnest(c(data, z_smooth))\n\nggplot() +\n  geom_line(\n    data = df_plot |> mutate(z_alpha = rnorm(n(), p_alpha, p_alpha_sd)),\n    aes(xn, zn, group = y_rank, alpha = z_alpha), na.rm = TRUE) +\n  geom_line(\n    data = df_smooth,\n    aes(xn, z_smooth, group = y_rank), alpha = p_alpha) +\n  scale_alpha_identity() + coord_fixed() + theme_void()"
  },
  {
    "objectID": "posts/dispyr/index.html#iterations",
    "href": "posts/dispyr/index.html#iterations",
    "title": "Dispyr",
    "section": "Iterations",
    "text": "Iterations\n\nAn example of 32 random iterations from this algorithm."
  },
  {
    "objectID": "posts/attractors/index.html",
    "href": "posts/attractors/index.html",
    "title": "Ridges",
    "section": "",
    "text": "Distinct and shifted lines visualized from digital elevation model datasets.\n\nThe central idea is to convert 3D point sets to 2D lines. Digital Elevation Model (DEM) represents elevation as function of latitude and longitude. In France, the national geographic institute makes this DEM available in Open-Data (RGE, up to a 1m resolution). These illustrations are made of multiple lines of elevation as a function of longitude, for discrete latitude values. Overplotting is avoided (hidden lines) by checking for a minimal distance between two lines. Variation comes from the scale and location of the geographical region and from the parameterization of the line clipping method.\nAdding noise or removing information from the original DEM data seems to create more a more natural rendering. This option was explored in the dispyr series"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paths",
    "section": "",
    "text": "A visual blog about algorithmic art, mainly designed for pen plotters.\n\n\n\nI am using the R programming language to write algorithms that generate digital outputs. Rather than printing, I am interested in using a pen-plotter to draw them on paper. While this process enables drawing with enhanced repetition and precision, it limits the range of techniques available (fills or hatching have to be programmed, pen pressure is constant). Somehow, i assume that the constraints imposed by this machine are a nice frame to the flexibility of code.\nThis site presents mostly digital outputs and some paper drawings, and documents the ideas underlying the functioning of the algorithms. Most of the code to study and reproduce these outputs is released on github . Overall, making visual art is very new to me, in my day job I am a scientist  working in plant biology.\nUnless stated otherwise, the art on this site is released under a    open license: you are free to reuse the work as long as you attribute the original to me, and derivative works must use the same licence.\nFeel free to contact me via email  for any questions related to this place or if you wish to order a plotter print from a work. I’m also somehow using social networks ( or ). You can also support me through the energy-efficient tezos blockchain, where I sometimes release works with the casadebaig.tez ns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDispyr\n\n\n\n\n\n\n\n\n\n\n\n\n\nCells\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollatz\n\n\n\n\n\n\n\n\n\n\n\n\n\nRidges\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsemic\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttractors\n\n\n\n\n\n\n\n\n\n\n\n\n\nPapers\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a scientist working for the French National Institute of Agricultural Research (INRAE). I use computational and modelling approaches to understand how crops interact with their environments. I develop tools concerning the agricultural extension sector, to improve the management of the cultivated genetic material. You can find additional information on my work and publications on my ORCID and scholar pages.\n\n\n\nI try to use my data science skills to create algorithmic art. I published these works on this website, with some posts in mastodon. You can also find some examples along with the tools i used for creative coding here."
  }
]